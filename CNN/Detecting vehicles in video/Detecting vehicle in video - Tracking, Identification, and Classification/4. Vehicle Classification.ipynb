{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Classification\n",
    "So far, we have extracted the vehicles from the frame and stored them in the folder. Next, we will perform the classification of vehicles present in the video. The training data is divided into 3 classes:\n",
    "\n",
    "1. 2/3 wheelers (motorbikes, rickshaws)\n",
    "2. 4 wheelers (cars)\n",
    "3. More-than-4 wheelers (buses, trucks, etc.)\n",
    "\n",
    "We have to manually label the data, in the sense that, you can make 3 folders for each of these 3 classes. And then, manually put the cropped images into one of the folders depending on the class.  Now your data is ready for training.\n",
    "Once done you can proceed forward with this notebook and train an algorithm to classify the vehicles into above mentioned three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are basically two ways how you can feed the data during training.\n",
    "- The first approach which is used here is to load all the images at once in the form of .npz file (zipped numpy array). Note that, the images are read as a numpy array. This can work if the memory used by the data is relatively less. Here, the size of the images are (50,50) which is a small size and the total dataset is 12k. So the dataset is relatively small.\n",
    "- The second approach is to load the dataset during training runtime using ImageDataGenerator function of Keras. Here, only a batch of data gets loaded into the memory.\n",
    "\n",
    "Here since dataset is small, we will take the first approach of storing files as npz format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "video = \"Vehicles\" #file name of npz\n",
    "mode = \"test\"\n",
    "file_name = \"demo.npz\"\n",
    "to_stack = False\n",
    "\n",
    "# Let's define a dictionary that maps the different classes we have to numbers\n",
    "dict = {'twoW':0, 'threeFourW':1, 'fivePlusW':2}\n",
    "for vehicle in sorted(dict):\n",
    "\t\n",
    "\tcount = 0\n",
    "\tarr_stack = np.zeros((1, 50, 50, 3))\n",
    "\ty = dict[vehicle] * np.ones((1, 1))\n",
    "\ts = np.array([dict[vehicle]])\n",
    "\ts = s[np.newaxis, :]\n",
    "\n",
    "\n",
    "\tfor i in range(100):\n",
    "\t\tpath = \"images/\"\n",
    "\t\timg_path = path + str(i) + \".jpg\"\t\n",
    "\t\ttry:\t\n",
    "\t\t\timg = Image.open(img_path)\n",
    "\t\t\tif count == 0:\n",
    "\t\t\t\tarr = np.array(img)\n",
    "\t\t\t\tarr_stack[0, :, :, :] = arr\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\telse: \n",
    "\t\t\t\tarr = np.array(img)\n",
    "\t\t\t\tarr_stack = np.concatenate((arr_stack, arr[np.newaxis, ...]), axis = 0)\n",
    "\t\t\t\ty = np.concatenate((y, s), axis = 0)\n",
    "\t\t\t\t# print arr_stack.shape, s\n",
    "\t\texcept IOError:\n",
    "\t\t\tprint (\"Error\")\n",
    "\n",
    "\tif to_stack:\n",
    "\t\tarr_test = np.load(file_name)\n",
    "\t\tarr_stack = np.concatenate((arr_stack, arr_test['a']), axis = 0)\n",
    "\t\ty = np.concatenate((y, arr_test['b']), axis = 0)\n",
    "\n",
    "\tnp.savez_compressed(video, a = arr_stack, b = y)\n",
    "\tto_stack = True\n",
    "arr_test = np.load(file_name)\n",
    "# print 'ff', arr_test['a'].shape, arr_test['b'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries to train the model\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "#from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import numpy\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining variables and dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nb_classes = 3\n",
    "nb_epoch = 300\n",
    "data_augmentation = True\n",
    "\n",
    "# We have resized images as 50, 50 when storing\n",
    "img_rows, img_cols = 50, 50\n",
    "img_channels = 3\n",
    "\n",
    "# We manually split data into train and test npz files and load them.\n",
    "arr = numpy.load(\"trainVehicles.npz\")\n",
    "X_train = 255 - arr['a']\n",
    "y_train = arr['b']\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "\n",
    "arr = numpy.load(\"testVehicles.npz\")\n",
    "X_test = 255 - arr['a']\n",
    "y_test = arr['b']\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train -= 0.5\n",
    "X_test -= 0.5\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training Code\n",
    "Save model after compiling it, in case you have to press ^C during the execution of fit function.\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same',\n",
    "                        input_shape=(img_rows, img_cols, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.75))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.75))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, W_regularizer=l2(0.1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, W_regularizer=l2(0.1)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#HERE: Save model after compiling it, in case you have to press ^C during the execution of fit function.\n",
    "model_json = model.to_json()\n",
    "with open(\"modelVehicles.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "filename = \"modelVehicles.h5\"\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callback_list = [checkpoint]\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "\t  callbacks=callback_list,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code to check accuracies of trained models on any dataset.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "\n",
    "arr = numpy.load(\"trainAundh.npz\")\n",
    "X_train = 255 - arr['a']\n",
    "y_train = arr['b']\n",
    "\n",
    "#Load the dataset for which you want to get the accuracy.\n",
    "arr = numpy.load(\"testAundh.npz\")\n",
    "X_test = 255 - arr['a']\n",
    "y_test = arr['b']\n",
    "X_display = numpy.copy(X_test)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train -= 0.5\n",
    "X_test -= 0.5\n",
    "\n",
    "path = '../data/'\n",
    "model_name = 'modelVehicles.json'\n",
    "\n",
    "json_file = open(model_name, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "\t      optimizer='adam',\n",
    "\t      metrics=['accuracy'])\n",
    "print(\"CNN Loaded\")\n",
    "loaded_model.summary()\n",
    "#Iterate over all the weights, run a forward pass, check accuracy\n",
    "for i in range(1, 6):\n",
    "\tweights_name = 'modelVehicles.h5'\n",
    "\tloaded_model.load_weights(weights_name)\n",
    "\tall_predictions = loaded_model.predict(X_test, batch_size = 32, verbose = 0) #Gives class probabilities\n",
    "\tall_predictions = numpy.argmax(all_predictions, axis = 1) #Finds max probability. That is the output class of the image.\n",
    "\tall_predictions = all_predictions[:, numpy.newaxis] #Reshape to y_test.shape\n",
    "\terror = all_predictions == y_test #Find correctly classified images\n",
    "\tacc = float(numpy.sum(error))\n",
    "\tacc /= y_test.shape[0]\n",
    "\tacc *= 100\n",
    "\tprint('accuracy', acc)\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Image and its class, optionally display accuracy.\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import numpy\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def decode_predictions(predictions, images):\n",
    "\t\n",
    "\t# Function to display image and its class.\n",
    "\t\t\n",
    "\tdict_map = {0:'twoW', 1:'threeFourW', 2:'fivePlusW'}\n",
    "\tnImages = predictions.shape[0]\n",
    "\tprint (nImages)\n",
    "\tfor i in range(nImages):\n",
    "\t\tprint(dict_map[predictions[i]] + \" detected\")\n",
    "\t\tplt.imshow(Image.fromarray(images[i]))\n",
    "\t\tplt.show()\n",
    "\n",
    "arr = numpy.load(\"trainAundh.npz\")\n",
    "X_train = 255 - arr['a']\n",
    "y_train = arr['b']\n",
    "\n",
    "#Change the file name if you want to load any other dataset.\n",
    "arr = numpy.load(\"testAundh.npz\")\n",
    "X_test = 255 - arr['a']\n",
    "X_display = arr['a']\n",
    "print('X_test', X_test.shape)\n",
    "y_test = arr['b']\n",
    "print('y_test', y_test.shape)\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_train -= 0.5\n",
    "X_test -= 0.5\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "# First, you load the model architecture which is present in the form of 'json' file. Then you load the trained model weights(.h5 file) into model architecture. \n",
    "json_file = open('modelVehicles.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"modelVehicles.h5\")\n",
    "print(\"CNN Loaded\")\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "rng = numpy.arange(1988, 1998)\n",
    "# Predicting on test dataset\n",
    "X_toPredict = X_test[rng, :, :, :]\n",
    "\n",
    "predictions = loaded_model.predict(X_toPredict, batch_size = 32, verbose = 0)\n",
    "predictions = numpy.argmax(predictions, axis = 1)\n",
    "print('predictions', predictions)\n",
    "print(predictions[:, numpy.newaxis].shape, y_test.shape)\n",
    "decode_predictions(predictions, X_display[rng, :, :, :])\n",
    "\n",
    "\n",
    "# Checks the accuracy on the curent test dataset, by using the method described in get_accuracy.py\n",
    "all_predictions = loaded_model.predict(X_test, batch_size = 32, verbose = 0)\n",
    "all_predictions = numpy.argmax(all_predictions, axis = 1)\n",
    "all_predictions = all_predictions[:, numpy.newaxis]\n",
    "error = all_predictions == y_test\n",
    "acc = float(numpy.sum(error))\n",
    "acc /= y_test.shape[0]\n",
    "acc *= 100\n",
    "print('accuracy', acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
